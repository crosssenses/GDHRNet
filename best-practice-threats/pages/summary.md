# Best Practices

The Global Digital Human Rights Network has set out to identify – through sectoral, platform-specific and state-specific studies – key best practices of platforms and platform governance. These cover five key regulatory areas.

### Make better rules: Rules have power and so does infrastructure

- Regulating infrastructure behind the platforms is a powerful normative vector.

- The regulation of infrastructure providers can lead to substantial collateral damage.

- App developers are particularly interested in clear communication with the app stores and concrete information about what content of the app led to the app store's actions.

- The User/Monthly active user (MAU) figure itself, if it is a fixed component of regulation, should be regularly reviewed and revised (if necessary, by a government-independent expert opinion) also to allow exclusions, economic development and growth potential.

- Cross-platforming, especially in the case of problematic content (e.g., illegal content, election-related content, etc.), should increasingly bring platforms into exchange with each other.

- Small and medium platforms should come into low-threshold contact with larger platforms, not only in "crisis situations".

- Content moderation needs to be understood more broadly and the differences artisanal, industrial and automated content moderation models need to be better understood.

### Ensure rights: rights matter and discrimination needs to be eliminated

- Online discrimination can have grave consequences for public safety and social inclusion and should be expressly addressed in international, legal and national regulations, and these sources of law should be harmonized.

- States, tech companies and NGOs should work together on raising awareness of the problem of discrimination online, so people can recognize discriminatory practices and know their rights.

- More research about online discrimination is needed so this practice can be recognized and better addressed.

- Tech companies ought to share best practices in detecting and avoiding discriminatory practices.

- Tech companies ought to cooperate on developing the automated systems of content control instead of developing parallel systems, which would be more cost efficient and result in more harmonized systems.

- Filtering algorithms would require human review to prevent human rights violations and discrimination.

- The existing mechanisms for reputational and copyright protection such as notice and take down procedures and the right to be forgotten can analogously be applied in case of online discrimination.

### Respect the rule of law: Platforms have to stick to rule of law standards

- There is a need for additional transparency measures for online platforms, including on the algorithms used. Platforms that feature user-generated content should offer users a clear explanation of their approach to evaluating and resolving reports of hateful and discriminatory content, highlighting their relevant terms of service.

- Greater ease for reporting cases of online discrimination (user-friendly mechanisms and procedures).

- Platforms should enforce sanctions of their terms of service in a consistent, timely and fair manner.

- Platforms should abide by duty of care, going beyond notice-and-takedown based legal models.

- Legislative framework for handling of requests to take down discriminatory content should be put in place.

- Procedural protections should be built into platforms notice-and-takedown systems.

- Rules should incentivize intermediaries and users to detect illegality, while minimizing the risks and the costs of errors and safeguarding a balance between the different human rights at stake.

- Tech companies need to ensure algorithm transparency and neutrality.

- A balance between citizens and tech companies must be struck in designing the liability rules.

- Setting up a detailed and harmonized European notice and take down procedure would provide more legal certainty.

### Make platforms more democratic: To make platforms more accountable, deliberative elements can be introduced

- Minimising associated risks: If not carefully designed, social media councils and other institutional proposals for renegotiating the relationship between societies, states and platforms, may conceal actual power structures and fail to initiate real change, providing only a perceived legitimacy while stabilising a status quo many societies seem uncontempt with.

- Design Requirements for New Institutions: Against the background of these risks, new institutional solutions have to meet the highest standards of transparency not just regarding their activities, but also regarding the systemic improvements they initiate at scale, being equipped with appropriate rights to information and data access to investigate and report on these aspects.

- Both media councils and social media councils are dependent on various stakeholders (government(s), public, companies, professionals etc).  Every stakeholder must trust that the council is working on their benefits and disagreements may be solved.

- Sanctions: Effective self-regulation organs must have competence to order sanctions. These sanctions do not need to be fines or measurable in money but sanctions must be credible and reason to change bad practices.

- To limit the unchecked exertion of power by large platforms over political discourses and individual expression, major platform companies should, in their private ordering systems and terms of service, refer at least partially to external standards which cannot be arbitrarily changed by these companies, and their processes should be equipped with institutional structures that negotiate the relationship between the two sets of rules.

- Online platforms should have independent bodies consisting of legal experts evaluating the reported cases of discrimination in order to achieve better balancing of rights.

- Platforms and domestic legal frameworks need to consider tackling responsibility for the consequences of regulatory choices: for online hate speech, focusing on impact in socio-legal terms means opting for substantive equality between groups

- Institutional measures (e.g. third-party reporting centres, state-non-state partnerships) can be helpful to reverse mistrust towards law enforcement authorities felt by members of religious/racial minorities and improve accessibility to the criminal legal system.

- It is important to encourage self-regulation of Internet portals that would make clear internal rules regarding the prohibition of hate speech in user-generated content.

- It is important to systematically improve preventive measures against hate speech, primarily in terms of educating citizens about the harmfulness of hate speech and its consequences.

### Make platform governance innovative: Normative sandboxes can help platforms (and regulators) innovate

- To incorporate the protection of "Digital rights" directly into the algorithm governing the platforms, in order to provide built-in operating mechanisms of trade negotiation and mediation (eg. Art. 25 of GDPR). In this sense, international guidelines and collection of best practices could help.

- Collective bargaining agreements: To include a more binding and specific regulation using collective agreements between union workers and employers’ associations, where possible according to the legal framework.

- Local arrangement and code of conduct: To support at a municipal or regional level, the adoption of local provisions or of voluntary codes that could improve awareness of social and digital rights among riders.

- Regulatory sandboxes and living labs: To establish provisional legal frameworks in order to experiment with new forms of regulations and models of interaction suitable to protect the "Digital Rights" of “platform workers, according to the concept of “regulatory sandbox” included in the EU proposal called “Artificial Intelligence Act” (articles 53 and 54).

- Institutional Experiments: To achieve a balance between platforms’ and states’ power over the behaviour on the internet requires a multiplicity of bold institutional experiments. The Meta (Facebook) Oversight Board is, despite some shortcomings, a noteworthy and already at least partially successful experiment in this regard, but should not be elevated to an archetype of a social media council or conceptually monopolise the space for – still needed – further institutional innovation.